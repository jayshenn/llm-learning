{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "markdown",
   "source": [
    "## 第 4 章 线性回归\n",
    "\n",
    "线性回归用一条“最合适的直线（或超平面）”去拟合数据，用来**预测数值**或者**分析变量之间的影响关系**。"
   ],
   "id": "5460c1ad0dd50143"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4.1 线性回归简介\n",
    "\n",
    "### 4.1.1 什么是线性回归\n",
    "\n",
    "**定义**\n",
    "\n",
    "线性回归假设因变量 $y$ 和自变量 $x_1, x_2, \\dots, x_n$ 存在近似线性关系：\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n\n",
    "$$\n",
    "\n",
    "- $\\beta_0$：截距，表示当所有自变量都为 0 时，模型给出的“基准值”\n",
    "- $\\beta_1, \\dots, \\beta_n$：系数，表示每个自变量对 $y$ 的影响大小和方向（正相关 / 负相关）\n",
    "\n",
    "模型的目标：**选出一组最合适的系数 $\\beta$**，让“预测值”尽量接近“真实值”。\n",
    "\n",
    "**一元线性回归**\n",
    "\n",
    "只有一个自变量 (x) 时：\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x\n",
    "$$\n",
    "\n",
    "就是在平面直角坐标系中，**找一条最合适的直线**。\n",
    "\n",
    "**多元线性回归**\n",
    "\n",
    "有多个自变量时：\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n\n",
    "$$\n",
    "\n",
    "这时几何上对应的是一个**超平面**，但理解上和一元情况是一样的：用线性的形式去拟合数据。\n",
    "\n",
    "**应用场景（直观理解）**\n",
    "\n",
    "* GDP 预测：用投资、消费、出口等指标预测 GDP 增长\n",
    "* 广告效果评估：分析 TV、Radio、Newspaper 投放对销售额的影响\n",
    "* 药物剂量研究：剂量变化会让血压、血糖变化多少\n",
    "* 产品质量控制：工艺参数（温度、压力等）对产品强度、寿命的影响\n",
    "* 政策效果评估：最低工资、环保政策对就业率、排放量的影响\n",
    "* 气候变化：排放量、森林覆盖率等对气温的影响\n",
    "\n",
    "一句话总结：**线性回归用来回答“这个量变一点，那个量会大概变多少？”的问题。**"
   ],
   "id": "871c17b7749b0a8d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.1.2 线性回归 API 使用示例（sklearn）\n",
    "\n",
    "需求：\n",
    "某中学教师想研究**学生每周学习时间（小时）**和**数学成绩（0–100 分）**之间的关系，并用模型来**预测成绩**。"
   ],
   "id": "bb5461d009595dbc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T05:19:26.813503Z",
     "start_time": "2025-11-14T05:19:25.391429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 自变量，每周学习时长（单位：小时）\n",
    "X = [[5], [8], [10], [12], [15], [3], [7], [9], [14], [6]]\n",
    "\n",
    "# 因变量，数学考试成绩（单位：分）\n",
    "y = [55, 65, 70, 75, 85, 50, 60, 72, 80, 58]\n",
    "\n",
    "# 1. 创建线性回归模型\n",
    "model = LinearRegression()\n",
    "\n",
    "# 2. 训练模型，让它“学会”从学习时长预测成绩\n",
    "model.fit(X, y)\n",
    "\n",
    "# 3. 查看学到的参数\n",
    "print(\"系数 beta1:\", model.coef_)        # 每多学 1 小时，成绩大约多多少分\n",
    "print(\"截距 beta0:\", model.intercept_)   # 不学习时（x=0）模型预估的基准分\n",
    "\n",
    "# 4. 预测：每周学习 11 小时，可能考多少分\n",
    "print(\"学习 11 小时的预测成绩:\", model.predict([[11]])[0])"
   ],
   "id": "bd4b76ae746172cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "系数 beta1: [2.87070855]\n",
      "截距 beta0: 41.45069393718042\n",
      "学习 11 小时的预测成绩: 73.02848794740687\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**结果说明**\n",
    "\n",
    "* `coef_`（系数）：大于 0 表示“多学一点，分数涨一点”；数值越大，影响越大\n",
    "* `intercept_`（截距）：没有学习时间时的“理论成绩”，只是模型拟合出来的一个数，不一定真实存在\n",
    "* `predict([[11]])`：输入 11 小时，输出预测成绩，比如大约 75–80 分\n",
    "\n",
    "**应用场景**\n",
    "\n",
    "当你有一组“输入–输出”的历史数据（学习时间 → 成绩），线性回归可以帮你**学出一个简单的公式**，用于**预测未来**和**解释影响关系**。"
   ],
   "id": "af304a3c304e5227"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4.2 线性回归求解\n",
    "\n",
    "### 4.2.1 损失函数：用来衡量“拟合得好不好”\n",
    "\n",
    "模型的预测值和真实值一般不会完全相同，需要一个指标来衡量误差，这个指标就是**损失函数**。\n",
    "\n",
    "设：\n",
    "\n",
    "* 样本数：$n$\n",
    "* 第 $i$ 个样本的真实值：$y_i$\n",
    "* 模型的预测值：$f(x_i)$\n",
    "\n",
    "#### 1）均方误差（MSE）\n",
    "\n",
    "> 回归问题中最常用的损失函数。\n",
    "\n",
    "$$\n",
    "\\mathrm{MSE} = \\frac{1}{n} \\sum_{i=1}^n \\bigl(f(x_i) - y_i\\bigr)^2\n",
    "$$\n",
    "\n",
    "特点：\n",
    "\n",
    "* **大误差惩罚更重**：误差会被平方，比如误差从 2 变 10，平方从 4 变 100\n",
    "* **是凸函数**：只有一个全局最小值，且处处可导，方便用解析解或梯度下降求解\n",
    "* 在线性回归中，对 MSE 最小化的方法就是**最小二乘法**\n",
    "* 几何意义：**让所有点到直线（或超平面）的“垂直距离”的平方和最小**\n",
    "\n",
    "#### 2）MSE 与极大似然估计（MLE）的关系（偏理论）\n",
    "\n",
    "假设真实数据满足：\n",
    "\n",
    "$$\n",
    "y_i = \\beta^\\top x_i + \\varepsilon_i\n",
    "$$\n",
    "\n",
    "其中误差项：\n",
    "\n",
    "* $\\varepsilon_i$：**独立同分布**并且服从正态分布\n",
    "\n",
    "$$\n",
    "\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "给定 $x_i$ 和参数 $\\beta$，$y_i$ 的条件概率为：\n",
    "\n",
    "$$\n",
    "p(y_i \\mid x_i; \\beta) =\n",
    "\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\n",
    "\\exp\\left(- \\frac{(y_i - \\beta^\\top x_i)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "全部样本的**似然函数**：\n",
    "\n",
    "$$\n",
    "L(\\beta) = \\prod_{i=1}^n p(y_i \\mid x_i; \\beta)\n",
    "$$\n",
    "\n",
    "取对数得到对数似然函数：\n",
    "\n",
    "$$\n",
    "\\ln L(\\beta) =\n",
    "-\\frac{n}{2}\\ln(2\\pi\\sigma^2)\n",
    "-\\frac{1}{2\\sigma^2}\n",
    "\\sum_{i=1}^n (y_i - \\beta^\\top x_i)^2\n",
    "$$\n",
    "\n",
    "最大化 $\\ln L(\\beta)$ 等价于最小化：\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^n (y_i - \\beta^\\top x_i)^2\n",
    "$$\n",
    "\n",
    "也就是最小化 MSE。\n",
    "\n",
    "**结论：**\n",
    "当误差服从高斯分布时，线性回归使用 MSE 作为损失函数是理论最优的选择。\n",
    "\n",
    "#### 3）平均绝对误差（MAE）\n",
    "\n",
    "> 数据中有大量异常值（outliers）时更稳健的选择。\n",
    "\n",
    "$$\n",
    "\\mathrm{MAE} = \\frac{1}{n} \\sum_{i=1}^n |f(x_i) - y_i|\n",
    "$$\n",
    "\n",
    "特点：\n",
    "\n",
    "* 对**异常值不敏感**：因为误差是线性惩罚，不会被平方放大\n",
    "* 对**小误差**的惩罚比 MSE 更弱\n",
    "* 常用于金融风险、鲁棒回归等对异常值敏感的场景\n"
   ],
   "id": "762eba225964a910"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.2.2 一元线性回归解析解（手算公式）\n",
    "\n",
    "考虑一元线性回归：\n",
    "\n",
    "$$\n",
    "f(x_i) = \\beta_0 + \\beta_1 x_i\n",
    "$$\n",
    "\n",
    "目标：最小化 MSE：\n",
    "\n",
    "$$\n",
    "\\mathrm{MSE} = \\frac{1}{n} \\sum_{i=1}^n (\\beta_0 + \\beta_1 x_i - y_i)^2\n",
    "$$\n",
    "\n",
    "对 $\\beta_0, \\beta_1$ 求偏导、令导数为 0，可以得到闭式解（推导过程略）：\n",
    "\n",
    "* 记：\n",
    "\n",
    "  $$\n",
    "  \\bar x = \\frac{1}{n} \\sum_{i=1}^n x_i,\\quad\n",
    "  \\bar y = \\frac{1}{n} \\sum_{i=1}^n y_i\n",
    "  $$\n",
    "\n",
    "* 则系数为：\n",
    "\n",
    "  $$\n",
    "  \\beta_1 =\n",
    "  \\frac{\\sum_{i=1}^n (x_i - \\bar x)(y_i - \\bar y)}\n",
    "  {\\sum_{i=1}^n (x_i - \\bar x)^2}, \\quad\n",
    "  \\beta_0 = \\bar y - \\beta_1 \\bar x\n",
    "  $$\n",
    "\n",
    "**例子：学习时间与成绩**\n",
    "\n",
    "用同样的数据：\n",
    "\n",
    "```python\n",
    "X = [5, 8, 10, 12, 15, 3, 7, 9, 14, 6]\n",
    "y = [55, 65, 70, 75, 85, 50, 60, 72, 80, 58]\n",
    "```\n",
    "\n",
    "计算得到大致结果（四舍五入）：\n",
    "\n",
    "* $\\beta_1 \\approx 2.87$：多学 1 小时，成绩大约多 2.87 分\n",
    "* $\\beta_0 \\approx 41.45$：理论上的“0 小时学习”成绩\n",
    "\n",
    "即模型为：\n",
    "\n",
    "$$\n",
    "\\hat y \\approx 41.45 + 2.87 x\n",
    "$$\n",
    "\n",
    "**应用场景**\n",
    "\n",
    "样本量不大、只有一个特征时，直接用公式就能快速算出最优直线，无需写循环或用优化算法。"
   ],
   "id": "d1a7c431e85d3615"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.2.3 正规方程法（矩阵形式的解析解）\n",
    "\n",
    "当特征数较多时，我们通常用矩阵写法。\n",
    "\n",
    "定义：\n",
    "\n",
    "* 特征矩阵 $X$（在第一列加上一列全 1 对应截距）：\n",
    "\n",
    "  $$\n",
    "  X = \\begin{bmatrix}\n",
    "  1 & x_{11} & x_{12} & \\dots & x_{1m} \\\n",
    "  1 & x_{21} & x_{22} & \\dots & x_{2m} \\\n",
    "  \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\n",
    "  1 & x_{n1} & x_{n2} & \\dots & x_{nm}\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "* 参数向量 $\\beta$：\n",
    "\n",
    "  $$\n",
    "  \\beta =\n",
    "  \\begin{bmatrix}\n",
    "  \\beta_0 \\\n",
    "  \\beta_1 \\\n",
    "  \\vdots \\\n",
    "  \\beta_m\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "* 目标向量 $y$：\n",
    "\n",
    "  $$\n",
    "  y =\n",
    "  \\begin{bmatrix}\n",
    "  y_1 \\\n",
    "  y_2 \\\n",
    "  \\vdots \\\n",
    "  y_n\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "预测写成矩阵形式：\n",
    "\n",
    "$$\n",
    "\\hat y = X \\beta\n",
    "$$\n",
    "\n",
    "MSE（忽略常数 $\\frac{1}{n})$可写成：\n",
    "\n",
    "$$\n",
    "J(\\beta) = |X\\beta - y|_2^2 = (X\\beta - y)^\\top (X\\beta - y)\n",
    "$$\n",
    "\n",
    "对 $\\beta$ 求导并令导数为 0，可得**正规方程**：\n",
    "\n",
    "$$\n",
    "X^\\top X \\beta = X^\\top y\n",
    "$$\n",
    "\n",
    "只要 $X^\\top X$ 可逆，就有解析解：\n",
    "\n",
    "$$\n",
    "\\beta = (X^\\top X)^{-1} X^\\top y\n",
    "$$\n",
    "\n",
    "> 正规方程适合：样本数较多但**特征数量不太大**的情况。\n",
    "> 特征太多时，求逆 $(X^\\top X)^{-1}$ 计算量很大，此时更适合用梯度下降等迭代优化方式。\n",
    "\n",
    "**sklearn 中的 LinearRegression（背后就是用解析解）**"
   ],
   "id": "c57f8088f2763fa4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T05:20:42.047212Z",
     "start_time": "2025-11-14T05:20:42.027543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 三个简单样本\n",
    "X = [[0, 3],\n",
    "     [1, 2],\n",
    "     [2, 1]]\n",
    "y = [0, 1, 2]\n",
    "\n",
    "model = LinearRegression(fit_intercept=True)  # 是否包含截距\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"系数 coef_:\", model.coef_)        # 对应每一列特征的系数\n",
    "print(\"截距 intercept_:\", model.intercept_)"
   ],
   "id": "95e0841876c495fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "系数 coef_: [ 0.5 -0.5]\n",
      "截距 intercept_: 1.4999999999999996\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**应用场景**\n",
    "\n",
    "当数据规模中等，且你希望**快速直接得到解析解**时，用 `LinearRegression` 非常方便。"
   ],
   "id": "3982e9aa2700927b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4.2.4 梯度下降法（数值优化思路）\n",
    "\n",
    "#### 1）基本更新公式\n",
    "\n",
    "梯度下降（Gradient Descent）是一种**迭代优化算法**。\n",
    "核心思想：**沿着损失函数的负梯度方向，一步一步往下走**。\n",
    "\n",
    "设目标函数为 $J(\\beta)$，参数为 $\\beta$，在第 $t$ 次迭代时：\n",
    "\n",
    "$$\n",
    "\\beta^{(t+1)} = \\beta^{(t)} - \\alpha \\cdot \\nabla J\\bigl(\\beta^{(t)}\\bigr)\n",
    "$$\n",
    "\n",
    "* $\\alpha$：学习率（learning rate），控制每一步走多大\n",
    "* $\\nabla J(\\beta)$：在 $\\beta$ 处的梯度（对各个参数的偏导）\n",
    "\n",
    "**在线性回归 + MSE 的场景中**，若使用矩阵形式\n",
    "$J(\\beta) = \\dfrac{1}{n}\\|X\\beta - y\\|_2^2$，可求得：\n",
    "\n",
    "$$\n",
    "\\nabla_\\beta J(\\beta) = \\frac{2}{n} X^\\top (X\\beta - y)\n",
    "$$\n",
    "\n",
    "这就是更新时要用的梯度。\n",
    "\n",
    "#### 2）计算示例（学习时间与成绩）\n",
    "\n",
    "仍然使用前面的数据，构造矩阵：\n",
    "\n",
    "* 原始特征：$x \\in \\mathbb{R}^{n \\times 1}$\n",
    "\n",
    "* 添加一列全 1 得到：\n",
    "\n",
    "  $$\n",
    "  X = \\begin{bmatrix}\n",
    "  1 & x_1 \\\\\n",
    "  1 & x_2 \\\\\n",
    "  \\vdots & \\vdots \\\\\n",
    "  1 & x_n\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "* 参数：\n",
    "\n",
    "  $$\n",
    "  \\beta =\n",
    "  \\begin{bmatrix}\n",
    "  \\beta_0 \\\\\n",
    "  \\beta_1\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "* 损失函数：\n",
    "\n",
    "  $$\n",
    "  J(\\beta) = \\frac{1}{n}\\|X\\beta - y\\|_2^2\n",
    "  $$\n",
    "\n",
    "* 梯度：\n",
    "\n",
    "  $$\n",
    "  \\nabla J(\\beta) = \\frac{2}{n} X^\\top (X\\beta - y)\n",
    "  $$\n",
    "\n",
    "选择：\n",
    "\n",
    "* 初始参数：$\\beta^{(0)} = [1, 1]^\\top$\n",
    "* 学习率：$\\alpha = 0.01$\n",
    "\n",
    "不断按公式更新：\n",
    "\n",
    "$$\n",
    "\\beta^{(t+1)} = \\beta^{(t)} - \\alpha \\cdot \\frac{2}{n} X^\\top (X\\beta^{(t)} - y)\n",
    "$$\n",
    "\n",
    "经过多次迭代后，$\\beta$ 会逐渐逼近解析解：\n",
    "$\\beta_0 \\approx 41.45,\\ \\beta_1 \\approx 2.87$，损失 $J(\\beta)$ 越来越小。\n",
    "\n",
    "---\n",
    "\n",
    "**实现代码示例（Numpy）**\n"
   ],
   "id": "358eef39c83fdc86"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T05:21:12.136092Z",
     "start_time": "2025-11-14T05:21:12.027386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# 原始数据\n",
    "X = np.array([[5], [8], [10], [12], [15], [3], [7], [9], [14], [6]])   # 学习时长\n",
    "y = np.array([[55], [65], [70], [75], [85], [50], [60], [72], [80], [58]])  # 成绩\n",
    "\n",
    "n = X.shape[0]\n",
    "\n",
    "# 在 X 前面加一列全 1，对应偏置 beta0\n",
    "X = np.hstack([np.ones((n, 1)), X])\n",
    "\n",
    "# 初始化参数 beta0, beta1 都为 1\n",
    "beta = np.array([[1.0], [1.0]])\n",
    "\n",
    "def J(beta):\n",
    "    \"\"\"目标函数：MSE\"\"\"\n",
    "    return np.sum((X @ beta - y) ** 2) / n\n",
    "\n",
    "def gradient(beta):\n",
    "    \"\"\"梯度\"\"\"\n",
    "    return 2 * X.T @ (X @ beta - y) / n\n",
    "\n",
    "alpha = 1e-2   # 学习率\n",
    "max_epoch = 10000\n",
    "\n",
    "for epoch in range(1, max_epoch + 1):\n",
    "    j = J(beta)\n",
    "    grad = gradient(beta)\n",
    "    beta = beta - alpha * grad\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"epoch={epoch}, beta={beta.reshape(-1)}, J={j}\")\n",
    "\n",
    "print(\"最终 beta:\", beta.reshape(-1))"
   ],
   "id": "38edcd3eee2034f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1000, beta=[39.30858775  3.07624954], J=3.6613222868937436\n",
      "epoch=2000, beta=[41.33570192  2.88174235], J=2.983114188688514\n",
      "epoch=3000, beta=[41.44452096  2.87130086], J=2.981159775470073\n",
      "epoch=4000, beta=[41.45036256  2.87074034], J=2.9811541433771502\n",
      "epoch=5000, beta=[41.45067615  2.87071025], J=2.981154127146987\n",
      "epoch=6000, beta=[41.45069298  2.87070864], J=2.981154127100208\n",
      "epoch=7000, beta=[41.45069389  2.87070855], J=2.981154127100079\n",
      "epoch=8000, beta=[41.45069393  2.87070855], J=2.981154127100084\n",
      "epoch=9000, beta=[41.45069394  2.87070855], J=2.981154127100067\n",
      "epoch=10000, beta=[41.45069394  2.87070855], J=2.981154127100069\n",
      "最终 beta: [41.45069394  2.87070855]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**学习率的影响**\n",
    "\n",
    "* 学习率过大：可能“跳过”最优点，甚至发散\n",
    "* 学习率过小：每次走得太小，收敛很慢\n",
    "* 实战中常用：\n",
    "\n",
    "  * 学习率衰减策略\n",
    "  * 自适应优化器（Adam、Adagrad 等）\n",
    "\n",
    "**实践建议**\n",
    "\n",
    "* 在使用梯度下降前，常常需要对特征做**标准化 / 归一化**，可以极大提升收敛速度\n",
    "* 对复杂损失面，可能会遇到局部最小值或鞍点，可以借助动量、Adam 等优化方法\n",
    "\n",
    "**sklearn 中用梯度方式训练线性回归：SGDRegressor**"
   ],
   "id": "b008ac445b31307b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T05:21:31.280184Z",
     "start_time": "2025-11-14T05:21:31.267969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "X = [[0, 3], [1, 2], [2, 1]]\n",
    "y = [0, 1, 2]\n",
    "\n",
    "model = SGDRegressor(\n",
    "    loss=\"squared_error\",   # 损失函数：平方误差\n",
    "    fit_intercept=True,     # 是否学习截距\n",
    "    learning_rate=\"constant\",\n",
    "    eta0=0.1,               # 初始学习率\n",
    "    max_iter=1000,\n",
    "    tol=1e-8                # 损失不再明显下降时提前停止\n",
    ")\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\"系数 coef_:\", model.coef_)\n",
    "print(\"截距 intercept_:\", model.intercept_)"
   ],
   "id": "ac546f66f7f6e5ad",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "系数 coef_: [ 0.90900056 -0.090926  ]\n",
      "截距 intercept_: [0.27286632]\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**应用场景**\n",
    "\n",
    "* 特征维度很高、样本巨大时（如推荐系统、CTR 预估），**正规方程求逆几乎不可行**，此时用 SGD/mini-batch SGD 等梯度方法更实际。"
   ],
   "id": "b88840ef578d9bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4.3 案例：广告投放效果预测\n",
    "\n",
    "### 4.3.1 数据集说明\n",
    "\n",
    "使用 Kaggle 上的 **Advertising 数据集**（`advertising.csv`）。其中字段有：\n",
    "\n",
    "* `ID`：样本序号（可删）\n",
    "* `TV`：电视广告投放金额（千元）\n",
    "* `Radio`：广播广告投放金额（千元）\n",
    "* `Newspaper`：报纸广告投放金额（千元）\n",
    "* `Sales`：销售额（百万元）\n",
    "\n",
    "目标：根据 TV、Radio、Newspaper 的投入，**预测销售额**。"
   ],
   "id": "99c38c2bf0a12318"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4.3.2 使用线性回归预测广告投放效果（完整代码）",
   "id": "484db1ae75b2b2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-14T05:22:01.748550Z",
     "start_time": "2025-11-14T05:22:01.709990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler          # 标准化\n",
    "from sklearn.model_selection import train_test_split      # 划分数据集\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error            # 评估：均方误差\n",
    "\n",
    "# 1. 加载数据集\n",
    "advertising = pd.read_csv(\"data/advertising.csv\")\n",
    "advertising.drop(advertising.columns[0], axis=1, inplace=True)  # 删除 ID 列\n",
    "advertising.dropna(inplace=True)                                # 删除缺失样本\n",
    "\n",
    "print(advertising.info())\n",
    "print(advertising.head())\n",
    "\n",
    "# 2. 划分特征和目标\n",
    "X = advertising.drop(\"Sales\", axis=1)   # TV, Radio, Newspaper\n",
    "y = advertising[\"Sales\"]                # 销售额\n",
    "\n",
    "# 划分训练集和测试集（70% 训练，30% 测试）\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=0\n",
    ")\n",
    "\n",
    "# 3. 特征标准化（很重要：有助于梯度下降收敛）\n",
    "preprocessor = StandardScaler()\n",
    "x_train = preprocessor.fit_transform(x_train)  # 用训练集计算均值和方差，并标准化\n",
    "x_test = preprocessor.transform(x_test)        # 使用相同的均值和方差标准化测试集\n",
    "\n",
    "# 4. 使用正规方程法（LinearRegression）\n",
    "normal_equation = LinearRegression()\n",
    "normal_equation.fit(x_train, y_train)\n",
    "\n",
    "print(\"正规方程法解得模型系数:\", normal_equation.coef_)\n",
    "print(\"正规方程法解得模型偏置:\", normal_equation.intercept_)\n",
    "\n",
    "# 5. 使用随机梯度下降法（SGDRegressor）\n",
    "gradient_descent = SGDRegressor(random_state=0)\n",
    "gradient_descent.fit(x_train, y_train)\n",
    "\n",
    "print(\"随机梯度下降法解得模型系数:\", gradient_descent.coef_)\n",
    "print(\"随机梯度下降法解得模型偏置:\", gradient_descent.intercept_)\n",
    "\n",
    "# 6. 使用均方误差评估两种模型在测试集上的表现\n",
    "y_pred_ne = normal_equation.predict(x_test)\n",
    "y_pred_gd = gradient_descent.predict(x_test)\n",
    "\n",
    "print(\"正规方程法均方误差:\", mean_squared_error(y_test, y_pred_ne))\n",
    "print(\"随机梯度下降法均方误差:\", mean_squared_error(y_test, y_pred_gd))"
   ],
   "id": "f9d30c3e511629f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200 entries, 0 to 199\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   TV         200 non-null    float64\n",
      " 1   Radio      200 non-null    float64\n",
      " 2   Newspaper  200 non-null    float64\n",
      " 3   Sales      200 non-null    float64\n",
      "dtypes: float64(4)\n",
      "memory usage: 6.4 KB\n",
      "None\n",
      "      TV  Radio  Newspaper  Sales\n",
      "0  230.1   37.8       69.2   22.1\n",
      "1   44.5   39.3       45.1   10.4\n",
      "2   17.2   45.9       69.3    9.3\n",
      "3  151.5   41.3       58.5   18.5\n",
      "4  180.8   10.8       58.4   12.9\n",
      "正规方程法解得模型系数: [3.68471841 3.05065643 0.03809335]\n",
      "正规方程法解得模型偏置: 14.355714285714287\n",
      "随机梯度下降法解得模型系数: [3.67804191 3.03289512 0.05802398]\n",
      "随机梯度下降法解得模型偏置: [14.33744876]\n",
      "正规方程法均方误差: 3.691394845698609\n",
      "随机梯度下降法均方误差: 3.6897573473002665\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**结果说明**\n",
    "\n",
    "* 两种方法（`LinearRegression` 和 `SGDRegressor`）本质上都是在拟合同一个线性模型，只是**求解方式不同**：\n",
    "\n",
    "  * `LinearRegression`：解析解（正规方程）\n",
    "  * `SGDRegressor`：迭代法（随机梯度下降）\n",
    "* 对比两个模型的 MSE，可以直观地看出：\n",
    "\n",
    "  * 在数据不太大时，解析解往往简单稳定\n",
    "  * 在大规模数据上，SGD 更灵活、可在线更新\n",
    "\n",
    "**应用场景总结**\n",
    "\n",
    "通过这个案例，我们可以用线性回归解决：\n",
    "\n",
    "> “不同广告渠道多投入一点，会让销售额多多少？”\n",
    "> “给定预算，怎样调整 TV/Radio/Newspaper 的投放结构，才能带来更高的销售？”\n",
    "\n",
    "这就是线性回归在商业场景中的典型用法。"
   ],
   "id": "84ae813d31cb33d8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
