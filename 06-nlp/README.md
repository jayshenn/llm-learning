# 第六阶段：NLP 自然语言处理 -- 人工智能皇冠上的明珠

## 学习目标

精通 NLP 核心技术和 Transformer 架构

## 核心内容

### NLP 基础
- 文本预处理、文本张量表示、数据增强

### 序列模型
- RNN、LSTM、GRU、Bi-LSTM

### 注意力机制
- Self-Attention、Multi-Head Attention、Seq2Seq

### Transformer
- 架构原理（编码器、解码器）
- 位置编码、多头注意力机制
- 前馈神经网络

### 预训练模型
- BERT（MLM、NSP、双向 Transformer）
- GPT（自回归语言模型）
- ELMO（上下文词向量）

### NLP 工具
- fastText、Transformers 库

## 目录结构

- `01-preprocessing/` - 文本预处理
- `02-transformer/` - Transformer
- `03-bert/` - BERT
- `04-project-ai-review/` - 项目二：AI 智评
- `05-project-knowledge-graph/` - 项目三：智荐图谱
- `06-project-optional/` - 项目四（三选一）

## 实战项目

1. **项目二：AI 智评**（深度学习技术）
2. **项目三：智荐图谱**（基于知识图谱的电商搜索和推荐）
3. **项目四**（三选一）：
   - 智选新闻（智能新闻分类、推荐和摘要）
   - 智医助手（智能医疗对话机器人）
   - AI 智教（在线教育智能分析平台）

## 学习资源

- [HuggingFace Transformers](https://huggingface.co/transformers/)
- 《自然语言处理综论》
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
