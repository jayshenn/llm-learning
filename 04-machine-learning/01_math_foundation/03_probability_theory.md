# 概率论

---

## 3.1 概率

### 3.1.1 概率的概念

**概率**是“某个事件发生有多大可能”的数字度量。

* 记号：事件 $A$ 的概率写作 $P(A)$
* 取值范围：$0 \le P(A) \le 1$

直观理解：

* $P(A) = 0$：事件几乎不可能发生（比如“明天太阳从西边升起”）
* $P(A) = 1$：事件必然发生（比如“明天会有 24 小时”）
* $P(A) = 0.5$：一半一半，像扔一枚公平硬币正面朝上

> **应用场景**：概率就是给“可能发生的事”分配一个 0 到 1 之间的数，用来量化不确定性。

---

### 3.1.2 概率的计算（基础公式）

常见概率关系可以记住下面几条：

1. **事件本身**

   * $0 \le P(A) \le 1$

2. **对立事件**

   * 事件 $A$ 不发生记作 $\overline{A}$（“非 A”）
   * 关系：
     $$P(\overline{A}) = 1 - P(A)$$

3. **联合概率（A 且 B）**

   * $A$ 和 $B$ 同时发生，记作 $A \cap B$
   * 一般情况：
     $$P(A \cap B) = P(A|B),P(B) = P(B|A),P(A)$$
   * 如果 $A$ 和 $B$ **相互独立**（互不影响），则：
     $$P(A \cap B) = P(A),P(B)$$

4. **并概率（A 或 B）**

   * “$A$ 或 $B$ 至少一个发生”，记作 $A \cup B$
   * 一般情况：
     $$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$
   * 如果 $A$ 和 $B$ **互斥（不可能同时发生）**，即 $P(A \cap B) = 0$，则：
     $$P(A \cup B) = P(A) + P(B)$$

5. **条件概率**

   * “在 $B$ 已经发生的前提下，$A$ 发生的概率”：
     $$P(A|B) = \frac{P(A \cap B)}{P(B)}$$

---

### 3.1.3 条件概率与联合概率例子：抽球

**题目：**

一个袋子里有 10 个球：6 个红球，4 个蓝球。随机抽取两个球（不放回）。

* 事件 $A$：第一个抽到的是红球
* 事件 $B$：两个抽到的球都是红球

#### 1）计算联合概率 $P(A \cap B)$

1. 第一个球是红球的概率：
   $$P(A) = \frac{6}{10}$$

2. 在第一个球是红球的前提下，第二个球也是红球的概率：
   $$P(B|A) = \frac{5}{9}$$
   因为已经拿走一个红球，剩下 5 个红球、9 个球。

3. 联合概率（两个球都是红球，且第一个是红球，本质上就是“两个球都是红球”）：
   $$P(A \cap B) = P(B|A),P(A) = \frac{5}{9} \times \frac{6}{10} = \frac{1}{3}$$

#### 2）计算条件概率 $P(A|B)$

$P(A|B)$：在已知两个球都是红球的情况下，第一个球是红球的概率。

1. 两个球都是红球的概率 $P(B)$：

   用组合数表示：

   * 从 6 个红球里选 2 个：$C_6^2$
   * 从 10 个球里选 2 个：$C_{10}^2$

   $$P(B) = \frac{C_6^2}{C_{10}^2} = \frac{15}{45} = \frac{1}{3}$$

2. 根据条件概率公式：
   $$P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{\frac{1}{3}}{\frac{1}{3}} = 1$$

**直观解释：**

既然已经知道“两个球都是红球”，那第一个球肯定是红球，概率自然就是 1。

> **应用场景**：条件概率适合回答这种“在已经知道某件事发生后，另一件事发生的可能性是多少”的问题。

---

## 3.2 概率分布

### 3.2.1 概率分布的概念

**随机变量**：用一个变量（比如 $X$）代表随机试验的结果，比如：

* 掷硬币：$X=0$ 表示反面，$X=1$ 表示正面
* 身高：$X$ 为某人的身高（连续值）

**概率分布**：描述“随机变量每个可能取值的概率”。

* 离散情况：列出每个取值和对应的概率
* 连续情况：用概率**密度函数** $f(x)$ 表示，在一个区间上的“密集程度”

一句话：**概率分布就是“完整地说明这个随机变量的可能结果，以及每个结果有多大可能”。**

---

### 3.2.2 均匀分布（Uniform Distribution）

**定义：**

在区间 $(a, b)$ 上，随机变量 $X$ 落在每个等长小区间里的概率都一样，这时就说 $X$ 服从 **均匀分布**，记作：

$$X \sim U(a, b)$$

**概率密度函数：**

$$
f(x) =
\begin{cases}
\frac{1}{b-a}, & a < x < b \\
0, & \text{其他}
\end{cases}
$$

直观理解：在 $[a,b]$ 上，“平均分布”，没有哪个位置特别偏好。

#### Python 示例：生成均匀分布数据

```python
import numpy as np

# 在区间 [0, 1] 上生成 10 个均匀分布的随机数
samples = np.random.uniform(0, 1, size=10)
print(samples)
```

你会看到 10 个 0 到 1 之间的随机小数，大致“均匀地散开”。

> **应用场景**：当你只知道一个值位于某个区间内、且没有任何额外偏好时，用均匀分布是最自然的假设。

---

### 3.2.3 正态分布（Normal Distribution）

**定义：**

正态分布（Normal / Gaussian）是最常见的连续分布之一，形状是对称的“钟形曲线”。

如果随机变量 $X$ 服从均值为 $\mu$、方差为 $\sigma^2$ 的正态分布，记作：

$$X \sim N(\mu, \sigma^2)$$

**概率密度函数：**

$$
f(x) = \frac{1}{\sigma\sqrt{2\pi}} ,
\exp\left( -\frac{(x-\mu)^2}{2\sigma^2} \right)
$$

* $\mu$：**期望 / 均值**，决定曲线的中心位置
* $\sigma^2$：**方差**，决定曲线“胖瘦”（$\sigma$ 越大，越“扁”）

**标准正态分布：**

* $\mu = 0, \ \sigma^2 = 1$
* 记作：$Z \sim N(0, 1)$

很多理论（比如中心极限定理）说明：大量独立随机因素的“和”或“平均值”会接近正态分布，所以现实中很多量（身高、误差等）都可以用正态分布近似。

#### Python 示例：生成正态分布数据

```python
import numpy as np

# 生成 1000 个服从 N(0, 1) 的随机数
samples = np.random.normal(loc=0.0, scale=1.0, size=1000)

print(samples[:10])  # 先看前 10 个
```

你会看到很多接近 0 的数，离 0 越远的数出现得越少。

> **应用场景**：正态分布常用来描述“很多小因素叠加在一起”的结果，比如测量误差、考试成绩、身高等。

---

## 3.3 贝叶斯定理

贝叶斯定理讲的是：**在获得新证据之后，如何更新我们对事件的看法（概率）**。

* 原来的看法叫 **先验概率**（prior）
* 加上证据之后更新的看法叫 **后验概率**（posterior）

---

### 3.3.1 全概率公式

很多时候，我们想算一个复杂事件 $B$ 的概率，但 $B$ 可能通过很多“路径”发生。

设有一组事件 $A_1, A_2, \dots, A_n$，满足：

* 两两互斥（不会同时发生）
* $P(A_1) + \dots + P(A_n) = 1$（一定有一个发生）

这组事件叫做一个 **完备事件组**。

如果我们知道：

* 每个 $A_i$ 的概率 $P(A_i)$
* 在 $A_i$ 发生的前提下，$B$ 发生的条件概率 $P(B|A_i)$

那么 $B$ 的总概率可以写成：

$$
P(B) = \sum_{i=1}^n P(B|A_i),P(A_i)
$$

这就叫 **全概率公式**。

> **应用场景**：当一个事件可以通过几种不同“情况”发生时，用全概率公式把每种情况的概率加起来。

---

### 3.3.2 贝叶斯公式

贝叶斯定理告诉我们：**已知 $B$ 发生后，如何更新 $A$ 的概率**。

基本形式（两个事件）：

$$
P(A|B) = \frac{P(B|A),P(A)}{P(B)}
$$

术语对应：

* $P(A)$：**先验概率**，我们在看到证据 $B$ 之前对 $A$ 的信念
* $P(B|A)$：**似然（likelihood）**，如果 $A$ 为真，看到 $B$ 的可能性
* $P(A|B)$：**后验概率**，在看到证据 $B$ 之后对 $A$ 的新看法
* $P(B)$：证据本身的概率，常用全概率公式计算

如果样本空间被互斥且完备的事件 $A_1, \dots, A_n$ 划分，则：

1. 先用全概率公式求证据的概率：
   $$
   P(B) = \sum_{i=1}^n P(B|A_i),P(A_i)
   $$
2. 再用贝叶斯公式求后验：
   $$
   P(A_k|B) = \frac{P(B|A_k),P(A_k)}{\sum_{i=1}^n P(B|A_i),P(A_i)}
   $$

---

### 3.3.3 贝叶斯公式例子：医疗检测

**题目：**

* 某疾病的发病率为 1%：
  $$P(\text{疾病}) = 0.01$$
* 有疾病时，检测呈阳性的概率为 95%：
  $$P(\text{阳性}|\text{疾病}) = 0.95$$
* 没有疾病时，检测仍呈阳性的概率为 5%（假阳性）：
  $$P(\text{阳性}|\text{无疾病}) = 0.05$$

现在某人检测结果为阳性，问：他 **真正患病** 的概率是多少？即 $P(\text{疾病}|\text{阳性})$。

#### 第一步：用全概率公式算 $P(\text{阳性})$

无疾病的概率：

$$P(\text{无疾病}) = 1 - 0.01 = 0.99$$

阳性的总概率：

$$
\begin{aligned}
P(\text{阳性})
&= P(\text{阳性}|\text{疾病}),P(\text{疾病})

* P(\text{阳性}|\text{无疾病}),P(\text{无疾病}) \\
  &= 0.95 \times 0.01 + 0.05 \times 0.99 \\
  &= 0.0095 + 0.0495 \\
  &= 0.059
  \end{aligned}
  $$

#### 第二步：用贝叶斯公式算 $P(\text{疾病}|\text{阳性})$

$$
P(\text{疾病}|\text{阳性})
= \frac{P(\text{阳性}|\text{疾病}),P(\text{疾病})}{P(\text{阳性})}
= \frac{0.95 \times 0.01}{0.059}
\approx 0.161
$$

也就是说，即使检测是阳性，他真正患病的概率也只有大约 **16.1%**。

**直观解释：**

* 这个病本身就很少见（1%）
* 检测虽然很准，但假阳性（5%）在大量健康人中也会产生很多“假病人”
* 所以“阳性 ≠ 一定有病”

> **应用场景**：贝叶斯定理常用于“根据证据重新评估可能性”，比如疾病诊断、垃圾邮件过滤、推荐系统等。

---

## 3.4 似然函数与极大似然估计

### 3.4.1 似然函数的概念

**概率（forward）**：已知参数，问数据的可能性。

* 比如：硬币正面概率是 $\theta = 0.5$，问“3 次中恰好 2 次正面”的概率是多少？

**似然（backward）**：已知数据，反过来推参数的可能性。

* 比如：观察到“3 次中 2 次正面”，问“硬币正面概率 $\theta$ 最可能是多少？”

形式上：

* 已知一个概率模型 $P(X|\theta)$

  * $X$：观测数据
  * $\theta$：模型参数
* **似然函数**定义为：
  $$
  L(\theta|X) = P(X|\theta)
  $$

这里强调：**把 $X$ 当作已知，把 $\theta$ 当作变量**。所以它是“关于 $\theta$ 的函数”。

如果有 $n$ 个独立同分布样本 $X = (x_1, \dots, x_n)$，则：

$$
L(\theta|X) = P(X|\theta) = \prod_{i=1}^n P(x_i|\theta)
$$

通常会用对数似然，把乘积变成和：

$$
\log L(\theta|X) = \sum_{i=1}^n \log P(x_i|\theta)
$$

这样更方便求导和优化。

> **应用场景**：似然函数是拿到数据后，用来衡量“在不同参数下，这些数据有多合理”的工具。

---

### 3.4.2 极大似然估计（Maximum Likelihood Estimation, MLE）

**极大似然估计**：从一堆可能的参数值中，选出 **让观测数据最可能出现** 的参数。

形式化目标：

$$
\hat{\theta}*{\text{MLE}} = \arg\max*{\theta} L(\theta|X)
$$

一般做法：

1. 写出似然函数 $L(\theta|X)$
2. 取对数得到 $\log L(\theta|X)$
3. 对 $\theta$ 求导，让导数等于 0，解出 $\hat{\theta}$
4. 检查是否为最大值（大多数经典问题是）

---

### 3.4.3 极大似然例子：三次掷硬币

**问题：**

掷硬币 3 次，观测到 **2 次正面，1 次反面**。能否估计硬币正面朝上的概率 $\theta$？

设：

* $\theta$：硬币正面朝上的概率
* 观测结果 $X$：2 次正面、1 次反面

1. **写出似然函数**

这是一个二项分布问题，$n=3, k=2$。给定 $\theta$，恰好 2 次正面的概率为：

$$
L(\theta|X) = P(X|\theta)
= C_3^2 \theta^2 (1-\theta)
$$

2. **取对数（对数似然）**

$$
\log L(\theta|X)
= \log C_3^2 + 2 \log \theta + \log(1-\theta)
$$

3. **求导并令导数为 0**

对 $\theta$ 求导：

$$
\frac{d}{d\theta} \log L(\theta|X)
= \frac{2}{\theta} - \frac{1}{1-\theta}
$$

令导数为 0：

$$
\frac{2}{\theta} - \frac{1}{1-\theta} = 0
$$

移项：

$$
\frac{2}{\theta} = \frac{1}{1-\theta}
\Rightarrow 2(1-\theta) = \theta
\Rightarrow 2 = 3\theta
\Rightarrow \theta = \frac{2}{3}
$$

所以极大似然估计为：

$$
\hat{\theta}_{\text{MLE}} = \frac{2}{3}
$$

**直观理解：**

* 一共试了 3 次，有 2 次正面
* 用“频率当概率”的想法，估计 $\theta \approx 2/3$
* 极大似然给出的结果和这个直观想法一致

> **应用场景**：极大似然估计是机器学习和统计建模里最常用的参数估计方法之一，用“让观测数据出现的可能性最大”来反推参数。
