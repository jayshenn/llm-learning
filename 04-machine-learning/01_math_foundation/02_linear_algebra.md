# 线性代数

---

## 2.1 标量与向量

### 2.1.1 标量与向量的概念

**标量（scalar）**

* 就是一个普通的数，只有“大小”没有“方向”。
* 比如：`3`、`-1.5`、`0.01` 都是标量。

**向量（vector）**

* 向量是**一串有顺序的数**，通常写成一列或一行。
* 有“大小”和“方向”。

常见两种写法：

* **列向量**（默认理解）：

  ```
  x = [2, 5, 8]^T
    = [2,
       5,
       8]
  ```

* **行向量**：

  ```
  x^T = [2, 5, 8]
  ```

> 应用场景：向量用来表示特征、坐标、参数，比如机器学习中一条样本就是一个向量。

---

### 2.1.2 向量的基本运算

下面用 Python + NumPy 做演示：

```python
import numpy as np

x = np.array([2, 5, 8])   # 行向量写法
y = np.array([1, 3, 7])
```

#### 1）向量转置

列向量转置得到行向量，反之亦然。

概念上：

* 列向量：`[2, 5, 8]^T`
* 行向量：`[2, 5, 8]`

在 NumPy 里，常见的做法是把一维向量 reshape 成列向量：

```python
x_col = x.reshape(-1, 1)   # 3×1 列向量
x_row = x_col.T            # 1×3 行向量
print(x_col.shape, x_row.shape)  # (3, 1) (1, 3)
```

> 场景：写公式时区分行/列很重要，尤其是在矩阵乘法里。

---

#### 2）向量相加

**规则：对应元素相加，长度必须相同。**

例子：

* `x = [2, 5, 8]`
* `y = [1, 3, 7]`
* `x + y = [2+1, 5+3, 8+7] = [3, 8, 15]`

代码：

```python
x = np.array([2, 5, 8])
y = np.array([1, 3, 7])
print(x + y)   # [ 3  8 15]
```

> 场景：组合两个特征向量、累计多个分量等。

---

#### 3）向量与标量相乘

**规则：标量乘以向量的每个元素。**

例子：`3 × [2, 5, 8] = [6, 15, 24]`

```python
x = np.array([2, 5, 8])
print(3 * x)  # [ 6 15 24]
```

> 场景：整体缩放一个向量，比如把速度加倍、把特征值缩放。

---

#### 4）向量内积（点乘）

**定义：**
两个同长度向量 `x, y` 的内积（dot product）是“对应元素乘积后再相加”，结果是**标量**。

例如：

* `x = [2, 5, 8]`
* `y = [1, 3, 7]`
* `x · y = 2*1 + 5*3 + 8*7 = 2 + 15 + 56 = 73`

```python
x = np.array([2, 5, 8])
y = np.array([1, 3, 7])
print(np.dot(x, y))  # 73
```

**与夹角的关系**

向量夹角 θ 的余弦：

`cos(θ) = (x · y) / (||x||_2 * ||y||_2)`

其中 `||x||_2` 是 `x` 的 L2 范数（见后面 2.1.3）。

```python
from numpy.linalg import norm

cos_theta = np.dot(x, y) / (norm(x) * norm(y))
print(cos_theta)
```

> 场景：余弦相似度在推荐系统、文本相似度中非常常见，用来衡量两个向量“方向”相似程度。

---

### 2.1.3 向量范数（norm）

**范数：** 把“向量有多大”这个直觉，变成一个数字的函数。
常用记法：`||x||`。

设 `x = [x1, x2, ..., xm]`。

---

#### 1）L0 范数（0 范数）

`||x||_0 = x 中非零元素的个数`

例子：`x = [0, 2, -1]`

* 非零元素有 `2` 和 `-1`
* 所以 `||x||_0 = 2`

```python
x = np.array([0, 2, -1])
l0 = np.count_nonzero(x)
print(l0)  # 2
```

> 场景：稀疏性度量（有多少非零特征）。

---

#### 2）L1 范数（和范数）

`||x||_1 = |x1| + |x2| + ... + |xm|`

例：`x = [0, 2, -1]`
`||x||_1 = |0| + |2| + |-1| = 3`

```python
from numpy.linalg import norm

x = np.array([0, 2, -1])
print(norm(x, ord=1))  # 3.0
```

> 场景：L1 正则化（Lasso）鼓励稀疏解。

---

#### 3）L2 范数（欧几里得范数）

`||x||_2 = sqrt(x1^2 + x2^2 + ... + xm^2)`

有时也会写 `||x||_2^2 = x1^2 + ... + xm^2`。

例：`x = [0, 2, -1]`
`||x||_2^2 = 0^2 + 2^2 + (-1)^2 = 5`
`||x||_2 = sqrt(5)`

```python
x = np.array([0, 2, -1])
print(norm(x))       # sqrt(5)
print(norm(x)**2)    # 5.0
```

> 场景：最常见的“长度”，几何距离、最小二乘等都用它。

---

#### 4）Lp 范数（一般形式）

`||x||_p = (|x1|^p + ... + |xm|^p)^(1/p)`，`p ≥ 1`

在 NumPy 中使用：

```python
x = np.array([0, 2, -1])
p = 3
lp = norm(x, ord=p)
print(lp)
```

> 场景：控制向量“大小”的不同方式，某些算法会用不同的 `p` 进行正则化或距离度量。

---

## 2.2 矩阵与张量

### 2.2.1 矩阵的概念

**矩阵（matrix）** 是一个“按行按列排成的数表”。
一个 `m×n` 的矩阵有 `m` 行、`n` 列。

例子：一个 3×2 矩阵：

```
A = [1 2
     3 5
     4 8]
```

我们通常写 `A ∈ R^{3×2}` 表示 `A` 是一个 3 行 2 列的实数矩阵。

#### 常见特殊矩阵

1）**方阵**：行数 = 列数。

```
[1 2
 3 4]  为 2×2 方阵
```

2）**对角矩阵**：主对角线以外元素全为 0 的方阵。

```
[1 0 0
 0 5 0
 0 0 9]
```

3）**单位矩阵**：主对角线上全是 1 的对角矩阵，记为 `I`。

```
I_3 = [1 0 0
       0 1 0
       0 0 1]
```

> 场景：矩阵表示线性变换、线性方程组、神经网络的权重等。

---

### 2.2.2 矩阵乘法

#### 1）矩阵乘法的定义与形状

设

* `A ∈ R^{m×n}`
* `B ∈ R^{n×p}`

才能定义 `AB`，结果是一个 `m×p` 的矩阵。

`(AB)_{ij} = sum_{k=1..n} A_{ik} * B_{kj}`

也就是：**A 的第 i 行 · B 的第 j 列**（向量内积）。

**例子**

```
A = [ 1  0  2
     -1  3  1]   (2×3)

B = [3  1
     2  1
     1  0]       (3×2)
```

计算：

* 第一行第一列：`1*3 + 0*2 + 2*1 = 5`
* 第一行第二列：`1*1 + 0*1 + 2*0 = 1`
* 第二行第一列：`-1*3 + 3*2 + 1*1 = 4`
* 第二行第二列：`-1*1 + 3*1 + 1*0 = 2`

所以：

```
AB = [5 1
      4 2]
```

代码：

```python
import numpy as np

A = np.array([[1, 0, 2],
              [-1, 3, 1]])
B = np.array([[3, 1],
              [2, 1],
              [1, 0]])

print(A @ B)
# [[5 1]
#  [4 2]]
```

**与单位矩阵相乘**

`AI = A`，`IA = A`（只要形状匹配）。

```python
I = np.eye(3)  # 3×3 单位矩阵
A = np.array([[1, 2],
              [3, 5],
              [4, 8]])  # 3×2
print(I @ A)  # 还是 A
```

> 场景：线性变换“串联”、神经网络多层权重相乘、线性方程组写成矩阵形式等。

---

#### 2）矩阵乘法的性质

设形状合法，则矩阵乘法满足：

* **结合律**：`(AB)C = A(BC)`
* **左分配律**：`A(B + C) = AB + AC`
* **右分配律**：`(A + B)C = AC + BC`

但**不满足交换律**：

* 一般情况下 `AB ≠ BA`

```python
A = np.array([[1, 2],
              [0, 1]])

B = np.array([[2, 0],
              [1, 3]])

print(A @ B)
print(B @ A)  # 两个结果通常不同
```

---

### 2.2.3 矩阵转置

#### 1）转置运算

矩阵 `A ∈ R^{m×n}` 的转置记为 `A^T`，是一个 `n×m` 的矩阵。

* `A^T` 的第 i 行 = `A` 的第 i 列
* `A^T_{ij} = A_{ji}`

例：

```
A = [1 2
     3 5
     4 8]   (3×2)

A^T = [1 3 4
       2 5 8]   (2×3)
```

```python
A = np.array([[1, 2],
              [3, 5],
              [4, 8]])

print(A.T)
```

#### 2）转置的性质

* `(A^T)^T = A`
* `(A + B)^T = A^T + B^T`
* `(kA)^T = kA^T`（k 是标量）
* `(AB)^T = B^T A^T`

> 场景：把“行向量”变成“列向量”、推导矩阵公式、实现某些对称性。

---

### 2.2.4 矩阵的逆

对于方阵 `A`，如果存在一个方阵 `A^{-1}`，使得：

* `AA^{-1} = I`
* 同时也有 `A^{-1}A = I`

就称 `A^{-1}` 为 `A` 的**逆矩阵**，`A` 可逆。

例子：
设

```
A = [1 2
     3 5]
A^{-1} = [-5  2
          3  -1]
```

可以验证：

```
A A^{-1} = [1*(-5) + 2*3    1*2 + 2*(-1)
            3*(-5) + 5*3    3*2 + 5*(-1)]
         = [1 0
            0 1]
         = I
```

NumPy 中计算逆矩阵：

```python
import numpy as np

A = np.array([[1, 2],
              [3, 5]])
A_inv = np.linalg.inv(A)
print(A_inv)
print(A @ A_inv)  # 接近单位矩阵（浮点运算有微小误差）
```

> 场景：线性方程组 `Ax = b` 的解可以写成 `x = A^{-1} b`（仅在 A 可逆时）。

---

### 2.2.5 其他常见矩阵运算

#### 1）矩阵的向量化（vec）

**向量化（vectorization）**：把矩阵按列“拉直”，变成一个列向量。

设：

```
A = [a11 a12 ... a1n
     a21 a22 ... a2n
     ...
     am1 am2 ... amn]
```

则：

* `vec(A)`：按列堆起来，得到一个 `mn×1` 列向量

  `vec(A) = [a11, ..., am1, a12, ..., am2, ..., a1n, ..., amn]^T`

* `rvec(A)`：按行堆起来，得到一个行向量

  `rvec(A) = [a11, ..., a1n, ..., am1, ..., amn]`

例：

* `A = [[1, 2], [3, 4]]`

则：

* `vec(A) = [1, 3, 2, 4]^T`
* `rvec(A) = [1, 2, 3, 4]`

```python
A = np.array([[1, 2],
              [3, 4]])

vecA = A.reshape(-1, 1, order="F")   # F: 按列
rvecA = A.reshape(1, -1, order="C")  # C: 按行
print(vecA.ravel())  # [1 3 2 4]
print(rvecA)         # [[1 2 3 4]]
```

> 场景：把矩阵当成长向量处理，比如做内积、拼接多个矩阵特征。

---

#### 2）矩阵的内积（Frobenius 内积）

两个同形矩阵 `A, B ∈ R^{m×n}` 的内积：

* `⟨A, B⟩ = sum_{i,j} A_{ij} * B_{ij}`
* 也可以写成：`⟨A, B⟩ = vec(A) · vec(B)`

```python
A = np.array([[1, 2],
              [3, 4]])
B = np.array([[2, 0],
              [1, 3]])

inner = np.sum(A * B)  # 对应元素相乘再求和
print(inner)
```

> 场景：定义矩阵的“长度”（Frobenius 范数）、衡量两个矩阵的相似程度。

---

#### 3）Hadamard 积（逐元素乘法）

两个同形矩阵 `A, B ∈ R^{m×n}` 的 Hadamard 积记为 `A ∘ B`（或者 `A ⨀ B`）：

* `(A ∘ B)_{ij} = A_{ij} * B_{ij}`

```python
A = np.array([[1, 2],
              [3, 4]])
B = np.array([[2, 0],
              [1, 3]])

print(A * B)
# [[2 0]
#  [3 12]]
```

> 场景：神经网络中的逐元素激活/门控、mask 操作等。

---

#### 4）Kronecker 积（⊗）

设

* `A ∈ R^{m×n}`
* `B ∈ R^{p×q}`

Kronecker 积 `A ⊗ B` 的形状是 `mp × nq`，构造方式是：

* 用 `A` 的每个元素 `a_ij` 去乘以矩阵 `B`，按原来 `A` 的布局拼成大矩阵。

例：

```
A = [a11 a12
     a21 a22]

A ⊗ B =
[ a11 B   a12 B
  a21 B   a22 B ]
```

简单代码示例：

```python
from scipy.linalg import kron  # 如果没有 scipy，可自己实现

A = np.array([[1, 2],
              [3, 4]])
B = np.array([[0, 5],
              [6, 7]])

K = kron(A, B)
print(K)
```

> 场景：构造大规模结构化矩阵、张量运算、某些图论与量子计算模型中会用到。

---

### 2.2.6 张量（tensor）

**张量**可以理解为“多维数组”：

* 0 阶张量：标量（如：`3`）
* 1 阶张量：向量（如：`[1, 2, 3]`）
* 2 阶张量：矩阵（如：`[[1, 2], [3, 4]]`）
* 3 阶及以上：更高维的“表格”，比如一堆矩阵堆起来。

例：一个 2×2×2 的 3 阶张量可以写成两张 2×2 矩阵堆叠：

```
T[:, :, 0] = [1 2
              3 5]

T[:, :, 1] = [3 2
              1 6]
```

在 NumPy 中：

```python
import numpy as np

T = np.array([
    [[1, 3],
     [3, 1]],
    [[5, 2],
     [6, 4]]
])
print(T.shape)  # (2, 2, 2)
```

> 场景：深度学习里的小批量图像 `(batch, height, width, channel)`，本质上就是一个高阶张量。

---

## 2.3 矩阵求导（矩阵微积分）

**核心思路：**

> 矩阵求导就是“对每个元素分别求偏导”，只是把结果重新组织成向量/矩阵的形式。

### 2.3.0 记号约定

* 向量变元：`x ∈ R^m`，通常看作列向量
  `x = [x1, x2, ..., xm]^T`
* 矩阵变元：`X ∈ R^{m×n}`
* 实标量函数：`f(x) ∈ R` 或 `f(X) ∈ R`
* 向量值函数：`f(x) ∈ R^p` 或 `f(X) ∈ R^p`

---

### 2.3.1 典型求导场景

#### （1）标量 f(x) 对向量 x 求导

`x = [x1, x2, ..., xm]^T`
`f(x)` 是一个标量，此时：

* `∂f(x)/∂x` 的结果是一个向量（和 x 同形）
* 实际上就是所有偏导数堆成一个向量：

`∂f(x)/∂x = [∂f/∂x1, ∂f/∂x2, ..., ∂f/∂xm]^T`

**例子**

`f(x1, x2) = x1^2 + x1 x2 + 2 x2^2`
令 `x = [x1, x2]^T`。

计算：

* `∂f/∂x1 = 2x1 + x2`
* `∂f/∂x2 = x1 + 4x2`

所以：

`∂f(x)/∂x = [2x1 + x2, x1 + 4x2]^T`

用 SymPy 验证：

```python
import sympy as sp

x1, x2 = sp.symbols('x1 x2')
f = x1**2 + x1 * x2 + 2 * x2**2

df_dx1 = sp.diff(f, x1)
df_dx2 = sp.diff(f, x2)
print(df_dx1, df_dx2)  # 2*x1 + x2   x1 + 4*x2
```

---

#### （2）标量 f(X) 对矩阵 X 求导

把矩阵 `X ∈ R^{m×n}` 看成由 `m*n` 个标量变量组成：

* 对 `X` 求导，就是对其中每个元素 `x_ij` 求偏导。
* 结果仍然是 `m×n` 形状的矩阵。

记为：

`(∂f(X)/∂X)_{ij} = ∂f / ∂x_{ij}`

也就是：

```
∂f/∂X =
[ ∂f/∂x11  ...  ∂f/∂x1n
  ...            ...
  ∂f/∂xm1  ...  ∂f/∂xmn ]
```

> 场景：损失函数对权重矩阵求梯度，比如线性层 `Wx + b` 中对 `W` 求导。

---

#### （3）向量 f(x) 对标量 x 求导

`f(x) = [f1(x), f2(x), ..., fp(x)]^T`，对标量 `x` 求导：

`∂f(x)/∂x = [df1/dx, df2/dx, ..., dfp/dx]^T`

例子：

* `f1(x) = 2x^2 + 3x + 1`
* `f2(x) = sin(x)`

则：

* `df1/dx = 4x + 3`
* `df2/dx = cos(x)`

所以：

`∂f(x)/∂x = [4x + 3, cos(x)]^T`

---

#### （4）向量 f(x) 对向量 x 求导（Jacobian）

仍然令：

`f(x) = [f1(x), f2(x), ..., fp(x)]^T`，其中 `x ∈ R^m`。

对向量 `x` 求导，会得到一个 `p×m` 的矩阵：

`(∂f/∂x)_{ij} = ∂fi/∂xj`

即：

```
∂f/∂x =
[ ∂f1/∂x1  ∂f1/∂x2  ...  ∂f1/∂xm
  ∂f2/∂x1  ∂f2/∂x2  ...  ∂f2/∂xm
  ...
  ∂fp/∂x1  ∂fp/∂x2  ...  ∂fp/∂xm ]
```

这个矩阵也叫 **Jacobian 矩阵**。

> 场景：多维函数的链式法则、自动微分框架的底层原理。

---

### 2.3.2 常用求导公式（记住就能直接用）

设：

* `x ∈ R^n`（列向量）
* `a ∈ R^n`（常向量）
* `A ∈ R^{m×n}` 或 `A ∈ R^{n×n}`（视情况而定）
* `X ∈ R^{m×n}`，`a ∈ R^m`，`b ∈ R^n`

以下求导结果在机器学习中非常常用：

1. `∂(x^T a)/∂x = a`
2. `∂(a^T x)/∂x = a`
3. `∂(x^T x)/∂x = 2x`
4. `∂(Ax)/∂x = A^T`（结果是矩阵/向量，视具体写法而定）
5. `∂(x^T A)/∂x = A`（形状注意配合）
6. `∂(x^T A x)/∂x = (A^T + A) x`
   若 `A` 对称，即 `A^T = A`，则有：`∂(x^T A x)/∂x = 2 A x`

与矩阵 `X` 相关的常见公式：

7. `∂(a^T X b)/∂X = a b^T`
8. `∂(a^T X^T b)/∂X = b a^T`
9. `∂(a^T X X^T b)/∂X = a b^T X + b a^T X`
10. `∂(a^T X^T X b)/∂X = X b a^T + X a b^T`

> 这些公式在推导线性回归、二次型最优化、深度学习反向传播时非常有用。

---

### 2.3.3 梯度与 Hessian

**1）梯度（gradient）**

* 对标量函数 `f(x)`，`x ∈ R^m`，梯度就是前面提到的 `∂f/∂x`：

`∇_x f(x) = ∂f/∂x = [∂f/∂x1, ..., ∂f/∂xm]^T`

* 对矩阵变元 `X ∈ R^{m×n}`，梯度是一个 `m×n` 的梯度矩阵：

```
∇_X f(X) = ∂f/∂X =
[ ∂f/∂x11  ...  ∂f/∂x1n
  ...
  ∂f/∂xm1  ...  ∂f/∂xmn ]
```

**2）Hessian（黑塞矩阵）**

* 对标量函数 `f(x)`，对 `x` 进行**二阶**求导：
* Hessian 是一个 `n×n` 的矩阵：

`H(x)_{ij} = ∂^2 f / (∂xi ∂xj)`

写成矩阵形式：

```
H(x) =
[ ∂^2f/∂x1^2       ∂^2f/∂x1∂x2   ...  ∂^2f/∂x1∂xn
  ∂^2f/∂x2∂x1      ∂^2f/∂x2^2    ...  ∂^2f/∂x2∂xn
  ...
  ∂^2f/∂xn∂x1      ∂^2f/∂xn∂x2   ...  ∂^2f/∂xn^2 ]
```

> 场景：梯度用在一阶优化（如梯度下降），Hessian 用在二阶方法（如牛顿法）、凸性判断等。
