# 第 1 章 机器学习概述

机器学习（Machine Learning, ML）关心的是：**给定一堆数据，能不能让计算机自己“摸索”出规律，然后对新数据做预测或分类**。

一个典型的机器学习过程是：

1. 收集大量带有标签的数据；
2. 选择一种模型（比如线性回归、决策树、神经网络）；
3. 用训练数据“喂”给模型，让模型自动学习参数；
4. 用训练好的模型，对从未见过的新数据进行预测。

简单数学上，可以把机器学习目标写成：

* 给定数据集  
  $D = \{(x_i, y_i)\}_{i=1}^N$

* 找到一个函数  
  $f(x; \theta)$  
  使得预测值 $\hat{y}_i = f(x_i; \theta)$ 尽量接近真实标签 $y_i$。

这里的 $\theta$ 就是模型的 **参数**，通过训练来学习。

机器学习是一门交叉学科，涉及概率论、统计学、线性代数、优化方法、算法设计等，但本教程会尽量用**例子+代码**来帮助理解，而不是推大量公式。

---

## 1.1 人工智能、机器学习与深度学习

### 1. 人工智能（AI）

人工智能是计算机科学的一个大领域，目标是让机器具备某种“智能行为”，比如理解语言、看图识物、规划路径、下棋等。

**关键点：**

* AI 是一个“总称”，不限定具体方法；
* 里面可以包含：

  * 规则系统（if-else 规则）；
  * 符号逻辑推理；
  * 统计方法；
  * 机器学习、深度学习等。

AI 是“让机器变聪明”的大框架，机器学习只是其中一种实现方法。

---

### 2. 机器学习（ML）

机器学习是 AI 的一个子领域，核心是**用数据驱动模型，让计算机自动从数据中学习规律，而且表现越来越好**。

* 不再完全依赖人工规则；
* 而是给机器大量“输入-输出”的样本，让它自己学出内部关系。

当你有大量“输入-输出”样本，想要自动学出“从输入到输出的映射”时，就需要机器学习。

---

### 3. 深度学习（DL）

深度学习是机器学习的一个子领域，使用**多层（深层）神经网络**来处理复杂任务，比如图像识别、语音识别、机器翻译等。

**特点：**

* 模型通常有很多层（几十层、上百层）；
* 自动学习“特征”，减少人工设计特征的工作；
* 非常适合处理图像、语音、文本这类复杂、高维的数据。

深度学习擅长处理“复杂、结构化”的数据，是当前图像、语音、自然语言处理的主流技术。

---

## 1.2 发展历史（了解）

这一节主要知道**从规则 → 统计 → 深度学习 → 大模型**的演变方向。

### 1.2.1 早期探索：20 世纪 50–70 年代

* 人工智能研究处于“推理期”，大家相信： **只要能让机器会逻辑推理，就能实现智能**。
* 重要事件：

  * 图灵提出“图灵测试”，作为判断机器是否“聪明”的标准；
  * Rosenblatt 提出感知机（Perceptron），是最早的神经网络之一，用于线性分类；
  * Arthur Samuel 提出“机器学习”一词，并为跳棋设计了学习算法。

早期主要靠 **逻辑推理和简单神经网络**，更多是理论探索。

---

### 1.2.2 知识驱动与专家系统：20 世纪 70–80 年代

* 研究者意识到：光有推理能力不够，机器必须**“知道很多知识”**；
* 出现大量基于“规则”的专家系统。

**典型内容：**

* 专家系统：如 MYCIN（做医疗诊断），把专家经验写成规则；
* 决策树算法开始出现，比如 ID3；
* 统计学习思想开始萌芽，贝叶斯方法逐渐被引入到 AI 中。

这个阶段是 **“知识工程 + 规则系统”** 的天下，用人类专家写规则来模拟智能。

---

### 1.2.3 数据驱动与统计学习：20 世纪 80 年代–21 世纪初

* 专家系统暴露出“知识工程瓶颈”：规则难以穷尽，维护困难；
* 研究重心转向**数据驱动**和**统计学习**。

**关键技术：**

* 决策树进一步发展：如 C4.5 算法；
* 支持向量机（SVM）成为强大的分类工具；
* 无监督学习发展：如 K-means 聚类；
* 集成学习方法兴起：随机森林、Boosting（如 AdaBoost）。

这一阶段开始强调 **“用数据说话”**，统计方法成为主流，机器学习正式独立成一门学科。

---

### 1.2.4 深度学习崛起：21 世纪初

* GPU 计算能力大幅提升；
* 互联网带来大量图像、语音、文本数据；
* 深度神经网络重获关注，并取得巨大成功。

**代表事件：**

* 深度信念网络（DBN）提出，标志着深度学习复兴；
* AlexNet 在 ImageNet 大赛中获胜，卷积神经网络（CNN）大放异彩；
* 生成对抗网络（GAN）提出，用于生成图像等；
* Transformer 架构在《Attention is All You Need》中提出，彻底改变 NLP 领域。

深度学习+大数据+高算力，让机器在视觉、语音、NLP 上的性能大幅超过传统方法。

---

### 1.2.5 大模型与通用人工智能：2020 年代

* 深度学习进入**规模化**阶段；
* 大语言模型（LLM）和多模态模型成为热点；
* 模型参数量巨大，追求更强的**泛化能力**和**通用性**。

**代表方向：**

* 自然语言处理：GPT 系列、BERT 等；
* 多模态模型：如 CLIP、Gato，将图像、文本等多种模态结合；
* 自监督学习：利用海量未标注数据，减少对人工标注的依赖；
* 深度强化学习应用于 AlphaGo、AlphaFold 等项目。

当前阶段的关键词是：**大规模、大模型、多模态、自监督**。

---

## 1.3 机器学习应用领域

今天几乎所有计算机相关方向都能看到机器学习的影子，常见的应用场景包括：

* **计算机视觉**：人脸识别、目标检测、图像分割、自动驾驶中的环境感知；
* **自然语言处理（NLP）**：机器翻译、文本分类、聊天机器人、情感分析；
* **语音处理**：语音识别、语音合成（TTS）、语音增强；
* **推荐系统**：电商推荐、短视频推荐、新闻推荐；
* **金融科技**：风控、信用评分、算法交易、欺诈检测；
* **医疗健康**：医学影像辅助诊断、疾病风险预测、蛋白质结构预测；
* **系统与工程**：芯片设计优化、网络流量预测、异常检测等。

只要有数据、有规律、需要预测或决策的地方，都可能用到机器学习。

---

## 1.4 基本术语

这一节非常重要，是后面所有内容的基础。建议先大致看一遍，有印象即可，后面遇到再回来看。

### 1.4.1 数据集（Data Set）

数据集是多条记录（样本）的集合，可以写成：

$$
D = \{(x_i, y_i)\}_{i=1}^N
$$

其中：

- $x_i$：第 $i$ 个样本的特征；
- $y_i$：对应的标签（监督学习中）。

在实践中，数据集通常会被拆成三部分：

* **训练集（Training Set）**：用来训练模型；
* **验证集（Validation Set）**：用来调节超参数、模型选择；
* **测试集（Test Set）**：用来最终评估模型性能。

正确划分数据集是评估模型是否“真的学会了，而不是死记训练数据”的前提。

---

### 1.4.2 样本（Sample）

数据集中的一条记录，描述一个事件或对象，就叫一个样本。

* 比如：一行“某用户的年龄、收入、是否买房”等信息；
* 每行就是一个样本。

样本是机器学习中的“最小单位”，模型就是在一堆样本上学习统计规律。

---

### 1.4.3 特征（Feature）

特征是描述样本某一方面性质的字段（一列数据）。

* 比如：年龄、身高、收入、像素点灰度值、单词出现次数等；
* 一个样本通常由多个特征组成。

特征是模型“看世界”的方式，特征设计好坏，直接影响模型表现。

---

### 1.4.4 特征向量（Feature Vector）

把一个样本的多个特征排成一个向量，记为特征向量，用于作为模型的输入。

* 对于一个样本，特征向量可以写作：

$$
x = [x_1, x_2, \dots, x_d]^\top
$$

其中，$d$ 是特征维度。

在代码中，你几乎所有“输入给模型的东西”都是特征向量或特征矩阵。

---

### 1.4.5 标签（Label）

在监督学习中，每个样本对应的“答案”或“结果”，叫做标签（也叫目标值 target）。

* 分类任务：标签是类别（如 0/1，猫/狗）；
* 回归任务：标签是连续数值（如房价、温度）。

标签是模型要“学会预测”的目标，没有标签就很难做监督学习。

---

### 1.4.6 模型（Model）

模型就是一个带参数的函数，用来把输入特征映射到输出。
可以理解为：**算法 + 训练后学到的参数**的组合。

数学上可以写成：

$$
\hat{y} = f(x;\theta)
$$

- $x$：特征向量  
- $\theta$：参数  
- $\hat{y}$：模型预测的输出

模型是整条机器学习流水线的“核心角色”，它决定了如何用数据学习规律。

---

### 1.4.7 参数（Parameter）

参数是模型在训练过程中学到的数值，比如线性回归中的权重和偏置。

* 对于线性回归：

$$
y = wx + b
$$

其中， $w$ 和 $b$ 就是参数。

* 对于神经网络：每一层的权重矩阵和偏置向量都是参数。

训练过程的目标，就是找到一组“最好的参数”，让模型在训练集和测试集上都表现良好。

---

### 1.4.8 超参数（Hyperparameter）

超参数是**在训练前由人或程序设定**的参数，模型不会自己学它们。

* 例子：

  * 学习率（learning rate）；
  * 正则化系数（比如 L2 惩罚项的系数）；
  * 决策树的最大深度；
  * 神经网络的层数和每层的神经元个数等。

调超参数是“模型调优”的关键步骤，往往对最终效果影响很大。

---

> 本章小结：
>
> * AI 是大框架，机器学习是其中的数据驱动方法，深度学习是机器学习的一种；
> * 历史从规则系统走向数据驱动和大模型；
> * 牢记：数据集、样本、特征、特征向量、标签、模型、参数、超参数这些基础术语，后面会不断出现。
